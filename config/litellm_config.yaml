model_list:
  # ‚úÖ Configuration Gemini (Principal)
  - model_name: gemini-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY
      
  - model_name: gemini-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GEMINI_API_KEY
      
  # ‚úÖ Configuration OpenAI (Fallback)
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY

# üõ°Ô∏è Configuration g√©n√©rale du proxy
general_settings:
  # Cl√© ma√Ætre pour l'administration
  master_key: sk-family-serve-master
  
  # Mode de d√©bogage pour le d√©veloppement
  set_verbose: true
  
  # Configuration des limites
  default_max_tokens: 2000
  default_model: gemini-pro
  
  # Fallback automatique en cas d'erreur
  fallbacks:
    - model: gemini-pro
      fallback: gpt-4o
    - model: gemini-flash  
      fallback: gpt-3.5-turbo

# üîß Param√®tres LiteLLM
litellm_settings:
  # Logging d√©taill√©
  set_verbose: true
  
  # Retry logic
  request_timeout: 600
  num_retries: 3
  
  # Cache des r√©ponses (optionnel)
  cache: true
  cache_type: redis  # Peut √™tre 'simple' pour un cache en m√©moire
  
# üìä Configuration du logging (optionnel)
success_callback: ["langfuse"]  # Ou ["lunary", "mlflow"] selon vos pr√©f√©rences

# ‚ö° Configuration du routeur pour la distribution de charge
router_settings:
  routing_strategy: "least-busy"  # ou "simple-shuffle", "latency-based"
  model_group_alias:
    gemini-models:
      - gemini-pro
      - gemini-flash